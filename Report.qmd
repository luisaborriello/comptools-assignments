---
title: "Lasso and Sparse Linear model recovery"
format: html
---
Lasso, differently from Ridge, performs variable selection; so generates simple models and models much easier to interpret than those produced by Ridge regression. Contrarily to Ridge which includes all p predictors in the model, Lasso yields sparse models, excludes irrelevant variables from a multiple regression model, which may contribute to add unuseful complexity into the model, involving only a subset of predictors and so guaranteeing model interpretability. Moreover, Lasso overcomes the disadvantage of Ridge, not only shrinking the coefficients estimated toward zero, like Ridge does, but also setting and forcing some of them to be exactly equal to zero, when the tuning parameter λ is sufficiently large, using an L1 penalty instead of a L2 penalty. Ridge, instead, also shrinks the coefficients toward zero but never set them equal to zero, except in the case in which λ is equal to infinity. We can also depict this difference between Lasso and Ridge graphically, considering the simplest case in which p=2, knowing that B̂ is the least squared coefficient estimates, and representing the lasso constraint and the ridge regression constraint, respectively, as a diamond and a circle. Each point of the specific ellipse or, in other words contour, around the least squared coefficient estimates, has the same RSS, and the more the ellipses expand away from the least squares coefficient estimates, the more the RSS increase. When the intersection between the ellipse and the Lasso diamond constraint, which has corners all around, occurs, one of the coefficients, β₁, equals zero. In addition, in case of higher dimensions, many coefficients simultaneously equal zero. Instead, if we consider Ridge case, there is no possible intersection between the ellipse and the circular ridge contraint, with no sharp points, and so the ridge coefficient estimates will be non-zero. The same consideration could be done in the case in which p is equal to 3, and the ridge regression constraint becomes a sphere, and the lasso contraint becomes a polyhedron, or in the case in which p is higher than 3, and the ridge constraint becomes a hypersphere and the lasso constraint a polytope. So, Lasso, like we said before, is preferred over Ridge in situations in which a simpler and more interpretable models are needed, with the inclusion of just a subset of predictors. However, also Lasso could have limits; differently from Ridge that could perform well in cases in which the response is a function of many predictors, all with coefficients of equal size, Lasso perform well just in case there is a setting with a relatively small number of predictors have substancial coefficients, and the remaining predictors have coefficients that are very small or equal to zero.




